{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daidao/anaconda3/envs/rl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from utils import ReplayBuffer, LinearSchedule\n",
    "import gym\n",
    "import pickle\n",
    "\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FCNet(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        # self.fc3 = nn.Linear(dims[2], dims[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4)\n",
    "        y = F.relu(self.fc1(x))\n",
    "        # y = F.relu(self.fc2(y))\n",
    "        y = self.fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, args, env):\n",
    "        self.args = args\n",
    "        self.online_network = FCNet([4, 50, 200, 2]) # Learns faster with a bigger network\n",
    "        self.target_network = FCNet([4, 50, 200, 2])\n",
    "        # args.network_fn = lambda optimizer_fn: DuelingFCNet([8, 50, 200, 2], optimizer_fn)\n",
    "        self.total_steps = 0\n",
    "        self.history_buffer = None\n",
    "        self.env = env\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.online_network.parameters(), lr=1e-3)\n",
    "        \n",
    "        \n",
    "    def save(self, file_name):\n",
    "        torch.save(self.online_network.state_dict(), file_name)\n",
    "        \n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_network.load_state_dict(self.online_network.state_dict())\n",
    "       \n",
    "    \n",
    "    def test_act(self, ob_var, eps, deterministic=False):\n",
    "        ob_var = ob_var.view(-1, 4)\n",
    "        q_out = self.online_network(ob_var)\n",
    "        _, deterministic_actions = q_out.max(1)\n",
    "        deterministic_actions = deterministic_actions.data\n",
    "\n",
    "        batch_size = ob_var.size(0)\n",
    "        random_actions = torch.LongTensor(batch_size).random_(0, 2)\n",
    "        choose_random = torch.Tensor(batch_size).uniform_(0, 1) < eps\n",
    "        \n",
    "        deterministic_actions[choose_random == 1] = 0 \n",
    "        random_actions[choose_random == 0] = 0\n",
    "        stochastic_actions = deterministic_actions + random_actions\n",
    "        out = stochastic_actions.cpu().numpy().astype(int).reshape(-1)\n",
    "        return out[0]\n",
    "    \n",
    "    \n",
    "    def act(self, ob_var, eps, deterministic=False):\n",
    "        action_value = self.online_network(ob_var)\n",
    "        action_value = action_value.cpu().data.numpy().flatten()\n",
    "        \n",
    "        if deterministic:\n",
    "            return np.argmax(action_value)\n",
    "        if np.random.rand() < eps:\n",
    "            return np.random.randint(0, len(action_value))\n",
    "        return np.argmax(action_value)\n",
    "        \n",
    "        \n",
    "    def test_update(self, obs, actions, rewards, next_obs, dones, weights):\n",
    "        obs = Variable(torch.from_numpy(obs).type(torch.FloatTensor)).view(-1, 4)\n",
    "        next_obs = Variable(torch.from_numpy(next_obs).type(torch.FloatTensor)).view(-1, 4)\n",
    "        dones = Variable(torch.from_numpy(dones.astype(float)).type(torch.FloatTensor)).view(-1, 1)\n",
    "        rewards = Variable(torch.from_numpy(rewards).type(torch.FloatTensor)).view(-1, 1)\n",
    "        actions = Variable(torch.from_numpy(actions.astype(int)).type(torch.LongTensor)).view(-1, 1)     \n",
    "\n",
    "        q_out = self.online_network(obs)\n",
    "        q_out_selected = q_out.gather(1, actions)\n",
    "\n",
    "        # Double Q, compute the Q-value of the next state based on the target network\n",
    "        # based on action chosen by the online network\n",
    "        next_q_out_online = self.online_network(next_obs).detach()\n",
    "        next_q_out_target = self.target_network(next_obs).detach()\n",
    "        _, next_q_best_online = next_q_out_online.max(1)\n",
    "        next_q_best = next_q_out_target.gather(1, next_q_best_online.view(-1, 1))\n",
    "\n",
    "        # compute RHS of bellman equation\n",
    "        # 0 Q-Value if episode terminates\n",
    "        next_q_best_masked = (1.0 - dones) * next_q_best\n",
    "        q_out_selected_target = rewards + self.args.discount * next_q_best_masked\n",
    "\n",
    "        # compute the error (potentially clipped)\n",
    "        loss = self.criterion(q_out_selected, q_out_selected_target)\n",
    "        # weighted_errors = (td_errors * weights).mean()\n",
    "\n",
    "        # Optimizer step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm(self.net.parameters(), self.args.grad_norm_clipping)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "    def update(self, obs, actions, rewards, next_obs, dones, weights):\n",
    "        obs = Variable(torch.from_numpy(obs).type(torch.FloatTensor)).view(-1, 4)\n",
    "        next_obs = Variable(torch.from_numpy(next_obs).type(torch.FloatTensor)).view(-1, 4)\n",
    "        dones = Variable(torch.from_numpy(dones.astype(float)).type(torch.FloatTensor)).view(-1, 1)\n",
    "        rewards = Variable(torch.from_numpy(rewards).type(torch.FloatTensor)).view(-1, 1)\n",
    "        actions = Variable(torch.from_numpy(actions.astype(int)).type(torch.LongTensor)).view(-1, 1)\n",
    "\n",
    "        # Compute Bellman loss -> DDQN\n",
    "        q_next = self.target_network(next_obs).detach()\n",
    "        _, best_actions = self.online_network(next_obs).detach().max(1)\n",
    "        q_next_best = q_next.gather(1, best_actions.view(-1, 1))\n",
    "        q_next_best_rhs = rewards + self.args.discount * q_next_best * (1 - dones)\n",
    "        # q_next_best_rhs.add_(rewards)\n",
    "        q = self.online_network(obs)\n",
    "        q = q.gather(1, actions).squeeze(1)\n",
    "        loss = self.criterion(q, q_next_best_rhs)\n",
    "        \n",
    "        # Step optimizer\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        self.discount = 0.99\n",
    "        self.test_repetitions = 50\n",
    "        self.double_q = True\n",
    "        self.test_interval = 100\n",
    "        self.test_repetitions = 50\n",
    "        self.success_threshold = 195\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.buffer_size = 50000\n",
    "        self.exploration_steps = 1000\n",
    "        self.max_timesteps = 100000\n",
    "        self.exploration_fraction = 0.1\n",
    "        self.exploration_final_eps = 0.02\n",
    "        self.learning_starts = 1000\n",
    "        self.train_freq = 1\n",
    "        self.target_network_update_freq = 500\n",
    "        self.print_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-10 20:09:41,356] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 197 episode 10 mean 100ep reward 22.0\n",
      "Step 426 episode 20 mean 100ep reward 22.5\n",
      "Step 638 episode 30 mean 100ep reward 22.0\n",
      "Step 872 episode 40 mean 100ep reward 22.4\n",
      "Step 1068 episode 50 mean 100ep reward 21.8\n",
      "Step 1223 episode 60 mean 100ep reward 20.7\n",
      "Step 1417 episode 70 mean 100ep reward 20.6\n",
      "Step 1599 episode 80 mean 100ep reward 20.3\n",
      "Step 1811 episode 90 mean 100ep reward 20.4\n",
      "Step 2048 episode 100 mean 100ep reward 20.7\n",
      "Step 2239 episode 110 mean 100ep reward 20.4\n",
      "Step 2461 episode 120 mean 100ep reward 20.4\n",
      "Step 2750 episode 130 mean 100ep reward 21.1\n",
      "Step 3101 episode 140 mean 100ep reward 22.3\n",
      "Step 3427 episode 150 mean 100ep reward 23.6\n",
      "Step 3763 episode 160 mean 100ep reward 25.4\n",
      "Step 4224 episode 170 mean 100ep reward 28.1\n",
      "Step 5171 episode 180 mean 100ep reward 35.7\n",
      "Step 6675 episode 190 mean 100ep reward 48.6\n",
      "Step 8450 episode 200 mean 100ep reward 64.0\n",
      "Step 10401 episode 210 mean 100ep reward 81.6\n",
      "Step 12396 episode 220 mean 100ep reward 99.4\n",
      "Step 14396 episode 230 mean 100ep reward 116.5\n",
      "Step 16396 episode 240 mean 100ep reward 133.0\n",
      "Step 18385 episode 250 mean 100ep reward 149.6\n",
      "Step 20371 episode 260 mean 100ep reward 166.1\n",
      "Step 22371 episode 270 mean 100ep reward 181.5\n",
      "Step 24371 episode 280 mean 100ep reward 192.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-39cbff401438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_starts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Do training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/pytorch-rl/tree_qn/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0midxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/research/pytorch-rl/tree_qn/utils.py\u001b[0m in \u001b[0;36m_encode_sample\u001b[0;34m(self, idxes)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mobses_tp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mdones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobses_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobses_tp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = Params()\n",
    "env = gym.make('CartPole-v0')\n",
    "agent = DQNAgent(args, env)\n",
    "\n",
    "# logger.configure('./log', ['csv', 'stdout'])\n",
    "exploration = LinearSchedule(schedule_timesteps=int(args.exploration_fraction * args.max_timesteps),\n",
    "                                    initial_p=1.0,\n",
    "                                    final_p=args.exploration_final_eps)\n",
    "replay_buffer = ReplayBuffer(args.buffer_size)\n",
    "episode_rewards = [0.0]\n",
    "saved_mean_reward = None\n",
    "reset = True\n",
    "ob = env.reset()\n",
    "\n",
    "# Copy params to target network\n",
    "agent.update_target()\n",
    "\n",
    "for t in range(args.max_timesteps):\n",
    "    # This callback is nicely done        \n",
    "    update_eps = exploration.value(t)\n",
    "    update_param_noise_threshold = 0.\n",
    "\n",
    "    ob_var = Variable(torch.from_numpy(ob).type(torch.FloatTensor))\n",
    "    # action = agent.act(ob_var, update_eps)\n",
    "    action = agent.test_act(ob_var, update_eps)\n",
    "\n",
    "    reset = False\n",
    "    new_ob, rew, done, _ = env.step(action)\n",
    "    # Store transition in the replay buffer.\n",
    "    replay_buffer.add(ob, action, rew, new_ob, float(done))\n",
    "    ob = new_ob\n",
    "\n",
    "    episode_rewards[-1] += rew\n",
    "    if done:\n",
    "        ob = env.reset()\n",
    "        episode_rewards.append(0.0)\n",
    "        reset = True\n",
    "\n",
    "    if t > args.learning_starts and t % args.train_freq == 0:\n",
    "        obs, actions, rewards, next_obs, dones = replay_buffer.sample(args.batch_size)\n",
    "        weights, batch_idxes = np.ones_like(rewards), None\n",
    "        # Do training\n",
    "        # agent.update(obs, actions, rewards, next_obs, dones, weights)\n",
    "        agent.test_update(obs, actions, rewards, next_obs, dones, weights)\n",
    "        \n",
    "        \n",
    "    if t > args.learning_starts and t % args.target_network_update_freq == 0:\n",
    "        # Update target network periodically.\n",
    "        agent.update_target()\n",
    "\n",
    "    if done and args.print_freq is not None and len(episode_rewards) % args.print_freq == 0:\n",
    "        mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
    "        num_episodes = len(episode_rewards)\n",
    "        print('Step {} episode {} mean 100ep reward {}'.format(t, num_episodes, mean_100ep_reward))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
