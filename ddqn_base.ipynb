{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from model import NoisyDuelingMLP\n",
    "from replay_buffer import PrioritizedReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DDQNAgent():\n",
    "    def __init__(self, args, env):\n",
    "        self.args = args\n",
    "        self.dtype = torch.FloatTensor\n",
    "        self.atype = torch.LongTensor\n",
    "        \n",
    "        self.model = NoisyDuelingMLP(4, 2, self.dtype, args.sigma_init)\n",
    "        self.target_model = NoisyDuelingMLP(4, 2, self.dtype, args.sigma_init)\n",
    "        self.env = env\n",
    "        self.huber_loss = nn.SmoothL1Loss(reduce=False)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "    \n",
    "    def sample_noise(self):\n",
    "        self.model.sample_noise()\n",
    "        self.target_model.sample_noise()\n",
    "        \n",
    "        \n",
    "    def reset_noise(self):\n",
    "        self.model.reset_noise()\n",
    "        self.target_model.reset_noise()\n",
    "        \n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "        \n",
    "    def act(self, ob):\n",
    "        ob_var = Variable(torch.from_numpy(ob).type(self.dtype)).view(-1, 4)\n",
    "        q_out = self.model(ob_var)\n",
    "        _, deterministic_actions = q_out.data.max(1)\n",
    "        out = deterministic_actions.cpu().numpy().astype(np.int32).reshape(-1)\n",
    "        return out[0]\n",
    "    \n",
    "    \n",
    "    def update(self, obs, actions, rewards, next_obs, dones, weights):\n",
    "        obs = Variable(torch.from_numpy(obs).type(torch.FloatTensor)).view(-1, 4)\n",
    "        next_obs = Variable(torch.from_numpy(next_obs).type(torch.FloatTensor)).view(-1, 4)\n",
    "        dones = Variable(torch.from_numpy(dones.astype(float)).type(torch.FloatTensor)).view(-1, 1)\n",
    "        rewards = Variable(torch.from_numpy(rewards).type(torch.FloatTensor)).view(-1, 1)\n",
    "        actions = Variable(torch.from_numpy(actions.astype(int)).type(torch.LongTensor)).view(-1, 1)\n",
    "        weights = Variable(torch.from_numpy(weights).type(torch.FloatTensor)).view(-1, 1)\n",
    "        \n",
    "        # Compute Bellman loss -> DDQN\n",
    "        q_next = self.target_model(next_obs).detach()\n",
    "        _, best_actions = self.model(next_obs).detach().max(1)\n",
    "        q_next_best = q_next.gather(1, best_actions.view(-1, 1))\n",
    "        q_next_best_rhs = rewards + self.args.gamma * q_next_best * (1 - dones)\n",
    "        q = self.model(obs)\n",
    "        q = q.gather(1, actions).squeeze(1)\n",
    "        \n",
    "        td_errors = q.data.view(-1, 1) - q_next_best_rhs.data.view(-1, 1)\n",
    "        errors = self.huber_loss(q, q_next_best_rhs)\n",
    "        weighted_error = (weights * errors).mean()\n",
    "        \n",
    "        # Step optimizer\n",
    "        self.optimizer.zero_grad()\n",
    "        weighted_error.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return td_errors.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        # Prioritized replay params\n",
    "        self.prioritized_replay = True\n",
    "        self.prioritized_replay_alpha = 0.5\n",
    "        self.prioritized_replay_beta0 = 0.4\n",
    "        self.prioritized_replay_beta_iters = None\n",
    "        self.prioritized_replay_eps = 1e-6\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.sigma_init = 0.17\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.buffer_size = 50000\n",
    "        self.exploration_steps = 1000\n",
    "        self.max_timesteps = 100000\n",
    "        self.exploration_fraction = 0.1\n",
    "        self.exploration_final_eps = 0.02\n",
    "        self.learning_starts = 1000\n",
    "        self.train_freq = 1\n",
    "        self.target_network_update_freq = 500\n",
    "        self.print_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-15 06:30:00,346] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 84 episode 10 mean 100ep reward 9.4\n",
      "Step 176 episode 20 mean 100ep reward 9.3\n",
      "Step 272 episode 30 mean 100ep reward 9.4\n",
      "Step 365 episode 40 mean 100ep reward 9.4\n",
      "Step 458 episode 50 mean 100ep reward 9.4\n",
      "Step 552 episode 60 mean 100ep reward 9.4\n",
      "Step 645 episode 70 mean 100ep reward 9.4\n",
      "Step 738 episode 80 mean 100ep reward 9.4\n",
      "Step 829 episode 90 mean 100ep reward 9.3\n",
      "Step 927 episode 100 mean 100ep reward 9.4\n",
      "Step 1035 episode 110 mean 100ep reward 9.5\n",
      "Step 1141 episode 120 mean 100ep reward 9.6\n",
      "Step 1242 episode 130 mean 100ep reward 9.7\n",
      "Step 1341 episode 140 mean 100ep reward 9.8\n",
      "Step 1447 episode 150 mean 100ep reward 9.9\n",
      "Step 1548 episode 160 mean 100ep reward 10.0\n",
      "Step 1646 episode 170 mean 100ep reward 10.0\n",
      "Step 1749 episode 180 mean 100ep reward 10.1\n",
      "Step 1855 episode 190 mean 100ep reward 10.3\n",
      "Step 1955 episode 200 mean 100ep reward 10.3\n",
      "Step 2056 episode 210 mean 100ep reward 10.2\n",
      "Step 2155 episode 220 mean 100ep reward 10.1\n",
      "Step 2252 episode 230 mean 100ep reward 10.1\n",
      "Step 2350 episode 240 mean 100ep reward 10.1\n",
      "Step 2448 episode 250 mean 100ep reward 10.0\n",
      "Step 2546 episode 260 mean 100ep reward 10.0\n",
      "Step 2645 episode 270 mean 100ep reward 10.0\n",
      "Step 2740 episode 280 mean 100ep reward 9.9\n",
      "Step 2843 episode 290 mean 100ep reward 9.9\n",
      "Step 2942 episode 300 mean 100ep reward 9.9\n",
      "Step 3044 episode 310 mean 100ep reward 9.9\n",
      "Step 3141 episode 320 mean 100ep reward 9.9\n",
      "Step 3241 episode 330 mean 100ep reward 9.9\n",
      "Step 3339 episode 340 mean 100ep reward 9.9\n",
      "Step 3440 episode 350 mean 100ep reward 9.9\n",
      "Step 3535 episode 360 mean 100ep reward 9.9\n",
      "Step 3631 episode 370 mean 100ep reward 9.9\n",
      "Step 3724 episode 380 mean 100ep reward 9.8\n",
      "Step 3825 episode 390 mean 100ep reward 9.8\n",
      "Step 3931 episode 400 mean 100ep reward 9.9\n",
      "Step 4039 episode 410 mean 100ep reward 10.0\n",
      "Step 4149 episode 420 mean 100ep reward 10.1\n",
      "Step 4273 episode 430 mean 100ep reward 10.3\n",
      "Step 4440 episode 440 mean 100ep reward 11.0\n",
      "Step 4606 episode 450 mean 100ep reward 11.7\n",
      "Step 4759 episode 460 mean 100ep reward 12.2\n",
      "Step 4877 episode 470 mean 100ep reward 12.5\n",
      "Step 5020 episode 480 mean 100ep reward 13.0\n",
      "Step 5141 episode 490 mean 100ep reward 13.2\n",
      "Step 5266 episode 500 mean 100ep reward 13.4\n",
      "Step 5381 episode 510 mean 100ep reward 13.4\n",
      "Step 5526 episode 520 mean 100ep reward 13.8\n",
      "Step 5656 episode 530 mean 100ep reward 13.8\n",
      "Step 5798 episode 540 mean 100ep reward 13.6\n",
      "Step 5950 episode 550 mean 100ep reward 13.4\n",
      "Step 6108 episode 560 mean 100ep reward 13.5\n",
      "Step 6266 episode 570 mean 100ep reward 13.9\n",
      "Step 6494 episode 580 mean 100ep reward 14.7\n",
      "Step 6744 episode 590 mean 100ep reward 16.0\n",
      "Step 7218 episode 600 mean 100ep reward 19.5\n",
      "Step 7851 episode 610 mean 100ep reward 24.7\n",
      "Step 8701 episode 620 mean 100ep reward 31.8\n",
      "Step 9522 episode 630 mean 100ep reward 38.7\n",
      "Step 10414 episode 640 mean 100ep reward 46.2\n",
      "Step 11473 episode 650 mean 100ep reward 55.2\n",
      "Step 12730 episode 660 mean 100ep reward 66.2\n",
      "Step 13984 episode 670 mean 100ep reward 77.2\n",
      "Step 15373 episode 680 mean 100ep reward 88.8\n",
      "Step 16531 episode 690 mean 100ep reward 97.9\n",
      "Step 17781 episode 700 mean 100ep reward 105.6\n",
      "Step 18977 episode 710 mean 100ep reward 111.3\n",
      "Step 20224 episode 720 mean 100ep reward 115.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-162c5e0300dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mkl_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_priorities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkl_errors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research/pytorch-rl/tree_qn/replay_buffer.py\u001b[0m in \u001b[0;36mupdate_priorities\u001b[0;34m(self, idxes, priorities)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \"\"\"\n\u001b[1;32m    322\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpriorities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriority\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriorities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mpriority\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = Params()\n",
    "env = gym.make('CartPole-v0')\n",
    "agent = DDQNAgent(args, env)\n",
    "\n",
    "replay_buffer = PrioritizedReplayBuffer(args.buffer_size, alpha=args.prioritized_replay_alpha)\n",
    "args.prioritized_replay_beta_iters = args.max_timesteps\n",
    "beta_schedule = LinearSchedule(args.prioritized_replay_beta_iters, \n",
    "                                initial_p=args.prioritized_replay_beta0, \n",
    "                                final_p=1.0)\n",
    "\n",
    "episode_rewards = [0.0]\n",
    "saved_mean_reward = None\n",
    "agent.sample_noise()\n",
    "agent.update_target()\n",
    "ob = env.reset()\n",
    "\n",
    "for t in range(args.max_timesteps):\n",
    "    action = agent.act(ob)\n",
    "    \n",
    "    new_ob, rew, done, _ = env.step(action)\n",
    "    # Store transition in the replay buffer.\n",
    "    replay_buffer.add(ob, action, rew, new_ob, float(done))\n",
    "    ob = new_ob\n",
    "    \n",
    "    episode_rewards[-1] += rew\n",
    "    if done:\n",
    "        ob = env.reset()\n",
    "        episode_rewards.append(0.0)\n",
    "        reset = True\n",
    "        \n",
    "    if t > args.learning_starts and t % args.train_freq == 0:\n",
    "        experience = replay_buffer.sample(args.batch_size, beta=beta_schedule.value(t))\n",
    "        (obs, actions, rewards, obs_next, dones, weights, batch_idxes) = experience\n",
    "        agent.sample_noise()\n",
    "        kl_errors = agent.update(obs, actions, rewards, obs_next, dones, weights)\n",
    "        replay_buffer.update_priorities(batch_idxes, np.abs(kl_errors) + 1e-6)\n",
    "        \n",
    "        \n",
    "    if t > args.learning_starts and t % args.target_network_update_freq == 0:\n",
    "        # Update target network periodically.\n",
    "        agent.update_target()\n",
    "\n",
    "    if done and args.print_freq is not None and len(episode_rewards) % args.print_freq == 0:\n",
    "        mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
    "        num_episodes = len(episode_rewards)\n",
    "        print('Step {} episode {} mean 100ep reward {}'.format(t, num_episodes, mean_100ep_reward))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
